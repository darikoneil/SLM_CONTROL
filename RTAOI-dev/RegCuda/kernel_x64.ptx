//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-21373419
// Cuda compilation tools, release 8.0, V8.0.55
// Based on LLVM 3.4svn
//

.version 5.0
.target sm_20
.address_size 64

	// .globl	MatrixMulKernel

.visible .entry MatrixMulKernel(
	.param .u64 MatrixMulKernel_param_0,
	.param .u64 MatrixMulKernel_param_1,
	.param .u64 MatrixMulKernel_param_2
)
{
	.reg .f32 	%f<12>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [MatrixMulKernel_param_0];
	ld.param.u64 	%rd2, [MatrixMulKernel_param_1];
	ld.param.u64 	%rd3, [MatrixMulKernel_param_2];
	cvta.to.global.u64 	%rd4, %rd3;
	cvta.to.global.u64 	%rd5, %rd1;
	cvta.to.global.u64 	%rd6, %rd2;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ctaid.y;
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %tid.y;
	shl.b32 	%r5, %r1, 14;
	shl.b32 	%r6, %r2, 5;
	add.s32 	%r7, %r6, %r5;
	shl.b32 	%r8, %r3, 9;
	add.s32 	%r9, %r7, %r8;
	add.s32 	%r10, %r9, %r4;
	mul.wide.s32 	%rd7, %r10, 8;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.f32 	%f1, [%rd8+4];
	ld.global.f32 	%f2, [%rd8];
	add.s64 	%rd9, %rd5, %rd7;
	ld.global.v2.f32 	{%f3, %f4}, [%rd9];
	mul.f32 	%f7, %f1, %f3;
	mul.f32 	%f8, %f2, %f4;
	mul.f32 	%f9, %f1, %f4;
	add.s64 	%rd10, %rd4, %rd7;
	sub.f32 	%f10, %f7, %f8;
	fma.rn.f32 	%f11, %f2, %f3, %f9;
	st.global.v2.f32 	[%rd10], {%f10, %f11};
	ret;
}

	// .globl	int2complex
.visible .entry int2complex(
	.param .u64 int2complex_param_0,
	.param .u64 int2complex_param_1
)
{
	.reg .f32 	%f<3>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [int2complex_param_0];
	ld.param.u64 	%rd2, [int2complex_param_1];
	cvta.to.global.u64 	%rd3, %rd1;
	cvta.to.global.u64 	%rd4, %rd2;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ctaid.y;
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %tid.y;
	shl.b32 	%r5, %r1, 14;
	shl.b32 	%r6, %r2, 5;
	add.s32 	%r7, %r6, %r5;
	shl.b32 	%r8, %r3, 9;
	add.s32 	%r9, %r7, %r8;
	add.s32 	%r10, %r9, %r4;
	mul.wide.s32 	%rd5, %r10, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.u32 	%r11, [%rd6];
	mul.wide.s32 	%rd7, %r10, 8;
	add.s64 	%rd8, %rd3, %rd7;
	cvt.rn.f32.s32	%f1, %r11;
	mov.f32 	%f2, 0f00000000;
	st.global.v2.f32 	[%rd8], {%f1, %f2};
	ret;
}

	// .globl	float2complex
.visible .entry float2complex(
	.param .u64 float2complex_param_0,
	.param .u64 float2complex_param_1
)
{
	.reg .f32 	%f<3>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [float2complex_param_0];
	ld.param.u64 	%rd2, [float2complex_param_1];
	cvta.to.global.u64 	%rd3, %rd1;
	cvta.to.global.u64 	%rd4, %rd2;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ctaid.y;
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %tid.y;
	shl.b32 	%r5, %r1, 14;
	shl.b32 	%r6, %r2, 5;
	add.s32 	%r7, %r6, %r5;
	shl.b32 	%r8, %r3, 9;
	add.s32 	%r9, %r7, %r8;
	add.s32 	%r10, %r9, %r4;
	mul.wide.s32 	%rd5, %r10, 4;
	add.s64 	%rd6, %rd4, %rd5;
	mul.wide.s32 	%rd7, %r10, 8;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.f32 	%f1, [%rd6];
	mov.f32 	%f2, 0f00000000;
	st.global.v2.f32 	[%rd8], {%f1, %f2};
	ret;
}

	// .globl	abs_of_complex
.visible .entry abs_of_complex(
	.param .u64 abs_of_complex_param_0,
	.param .u64 abs_of_complex_param_1
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<15>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [abs_of_complex_param_0];
	ld.param.u64 	%rd2, [abs_of_complex_param_1];
	cvta.to.global.u64 	%rd3, %rd2;
	cvta.to.global.u64 	%rd4, %rd1;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ctaid.y;
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %tid.y;
	shl.b32 	%r5, %r1, 14;
	shl.b32 	%r6, %r2, 5;
	add.s32 	%r7, %r6, %r5;
	shl.b32 	%r8, %r3, 9;
	add.s32 	%r9, %r7, %r8;
	add.s32 	%r10, %r9, %r4;
	mul.wide.s32 	%rd5, %r10, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.v2.f32 	{%f1, %f2}, [%rd6];
	abs.f32 	%f5, %f1;
	abs.f32 	%f6, %f2;
	setp.gt.f32	%p1, %f5, %f6;
	selp.f32	%f7, %f5, %f6, %p1;
	selp.f32	%f8, %f6, %f5, %p1;
	div.rn.f32 	%f9, %f8, %f7;
	fma.rn.f32 	%f10, %f9, %f9, 0f3F800000;
	sqrt.rn.f32 	%f11, %f10;
	mul.f32 	%f12, %f7, %f11;
	setp.eq.f32	%p2, %f7, 0f00000000;
	setp.gt.f32	%p3, %f7, 0f7F7FFFFF;
	or.pred  	%p4, %p2, %p3;
	setp.gt.f32	%p5, %f8, 0f7F7FFFFF;
	or.pred  	%p6, %p4, %p5;
	add.f32 	%f13, %f7, %f8;
	selp.f32	%f14, %f13, %f12, %p6;
	mul.wide.s32 	%rd7, %r10, 4;
	add.s64 	%rd8, %rd3, %rd7;
	st.global.f32 	[%rd8], %f14;
	ret;
}

	// .globl	sample_mean
.visible .entry sample_mean(
	.param .u64 sample_mean_param_0,
	.param .u32 sample_mean_param_1,
	.param .u32 sample_mean_param_2,
	.param .u32 sample_mean_param_3,
	.param .u32 sample_mean_param_4,
	.param .u64 sample_mean_param_5
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<48>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd4, [sample_mean_param_0];
	ld.param.u32 	%r17, [sample_mean_param_1];
	ld.param.u32 	%r20, [sample_mean_param_2];
	ld.param.u32 	%r18, [sample_mean_param_3];
	ld.param.u32 	%r19, [sample_mean_param_4];
	ld.param.u64 	%rd5, [sample_mean_param_5];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	mul.lo.s32 	%r21, %r20, %r17;
	mov.u32 	%r22, 1;
	sub.s32 	%r23, %r22, %r18;
	mad.lo.s32 	%r24, %r21, %r18, %r23;
	setp.ge.s32	%p1, %r4, %r24;
	@%p1 bra 	BB4_7;

	rem.s32 	%r25, %r4, %r18;
	setp.ne.s32	%p2, %r25, 0;
	@%p2 bra 	BB4_7;

	div.s32 	%r5, %r4, %r18;
	rem.s32 	%r6, %r5, %r17;
	div.s32 	%r7, %r5, %r17;
	mov.u32 	%r47, 0;
	mov.u32 	%r46, %r47;
	setp.lt.s32	%p3, %r18, 1;
	@%p3 bra 	BB4_5;

	cvta.to.global.u64 	%rd6, %rd4;
	mul.wide.s32 	%rd7, %r4, 2;
	add.s64 	%rd11, %rd6, %rd7;
	mov.u32 	%r47, 0;
	mov.u32 	%r46, %r47;
	mov.u32 	%r45, %r47;

BB4_4:
	ld.global.s16 	%r32, [%rd11];
	setp.gt.s32	%p4, %r32, 8192;
	add.s32 	%r33, %r46, %r32;
	add.s32 	%r34, %r33, -8192;
	selp.u32	%r35, 1, 0, %p4;
	add.s32 	%r47, %r47, %r35;
	selp.b32	%r46, %r34, %r46, %p4;
	add.s64 	%rd11, %rd11, 2;
	add.s32 	%r45, %r45, 1;
	setp.lt.s32	%p5, %r45, %r18;
	@%p5 bra 	BB4_4;

BB4_5:
	shr.u32 	%r36, %r7, 31;
	add.s32 	%r37, %r7, %r36;
	and.b32  	%r38, %r37, -2;
	sub.s32 	%r39, %r7, %r38;
	setp.eq.s32	%p6, %r39, %r19;
	shl.b32 	%r40, %r6, 1;
	add.s32 	%r41, %r17, %r5;
	add.s32 	%r42, %r41, -1;
	sub.s32 	%r43, %r42, %r40;
	selp.b32	%r16, %r5, %r43, %p6;
	setp.lt.s32	%p7, %r47, 1;
	@%p7 bra 	BB4_7;

	cvta.to.global.u64 	%rd8, %rd5;
	div.s32 	%r44, %r46, %r47;
	mul.wide.s32 	%rd9, %r16, 4;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.u32 	[%rd10], %r44;

BB4_7:
	ret;
}

	// .globl	sample_mean_debug
.visible .entry sample_mean_debug(
	.param .u64 sample_mean_debug_param_0,
	.param .u64 sample_mean_debug_param_1
)
{
	.reg .pred 	%p<9>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd2, [sample_mean_debug_param_0];
	ld.param.u64 	%rd3, [sample_mean_debug_param_1];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r6;
	setp.lt.s32	%p1, %r1, 786432;
	mul.hi.s32 	%r7, %r1, 1431655766;
	shr.u32 	%r8, %r7, 31;
	add.s32 	%r9, %r7, %r8;
	mul.lo.s32 	%r10, %r9, 3;
	sub.s32 	%r11, %r1, %r10;
	setp.eq.s32	%p2, %r11, 0;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB5_3;
	bra.uni 	BB5_1;

BB5_1:
	cvta.to.global.u64 	%rd4, %rd2;
	cvta.to.global.u64 	%rd5, %rd3;
	mul.wide.s32 	%rd6, %r1, 2;
	add.s64 	%rd7, %rd4, %rd6;
	ld.global.s16 	%r12, [%rd7];
	setp.gt.s32	%p4, %r12, 8192;
	add.s32 	%r13, %r12, -8192;
	selp.u32	%r14, 1, 0, %p4;
	selp.b32	%r15, %r13, 0, %p4;
	ld.global.s16 	%r16, [%rd7+2];
	setp.gt.s32	%p5, %r16, 8192;
	add.s32 	%r17, %r15, %r16;
	add.s32 	%r18, %r17, -8192;
	selp.b32	%r19, 2, 1, %p4;
	selp.b32	%r20, %r19, %r14, %p5;
	selp.b32	%r21, %r18, %r15, %p5;
	ld.global.s16 	%r22, [%rd7+4];
	setp.gt.s32	%p6, %r22, 8192;
	add.s32 	%r23, %r21, %r22;
	add.s32 	%r24, %r23, -8192;
	selp.u32	%r25, 1, 0, %p6;
	add.s32 	%r2, %r20, %r25;
	selp.b32	%r3, %r24, %r21, %p6;
	mul.hi.s32 	%r26, %r1, 715827883;
	shr.u32 	%r27, %r26, 31;
	shr.s32 	%r28, %r26, 8;
	add.s32 	%r29, %r28, %r27;
	shl.b32 	%r30, %r29, 9;
	sub.s32 	%r34, %r9, %r30;
	shr.s32 	%r35, %r26, 9;
	add.s32 	%r36, %r35, %r27;
	shl.b32 	%r37, %r36, 1;
	sub.s32 	%r38, %r29, %r37;
	setp.eq.s32	%p7, %r38, 1;
	shl.b32 	%r39, %r34, 1;
	add.s32 	%r40, %r9, 511;
	sub.s32 	%r41, %r40, %r39;
	selp.b32	%r42, %r9, %r41, %p7;
	mul.wide.s32 	%rd8, %r42, 4;
	add.s64 	%rd1, %rd5, %rd8;
	mov.u32 	%r43, 0;
	st.global.u32 	[%rd1], %r43;
	setp.lt.s32	%p8, %r2, 1;
	@%p8 bra 	BB5_3;

	cvt.rn.f32.s32	%f1, %r3;
	cvt.rn.f32.s32	%f2, %r2;
	div.rn.f32 	%f3, %f1, %f2;
	st.global.f32 	[%rd1], %f3;

BB5_3:
	ret;
}


